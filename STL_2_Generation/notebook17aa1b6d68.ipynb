{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy-stl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-24T16:14:14.376707Z","iopub.status.idle":"2024-06-24T16:14:28.188701Z","shell.execute_reply.started":"2024-06-24T16:14:14.377284Z","shell.execute_reply":"2024-06-24T16:14:28.187594Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting numpy-stl\n  Downloading numpy_stl-3.1.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from numpy-stl) (1.26.4)\nRequirement already satisfied: python-utils>=3.4.5 in /opt/conda/lib/python3.10/site-packages (from numpy-stl) (3.8.2)\nRequirement already satisfied: typing-extensions>3.10.0.2 in /opt/conda/lib/python3.10/site-packages (from python-utils>=3.4.5->numpy-stl) (4.9.0)\nDownloading numpy_stl-3.1.1-py3-none-any.whl (20 kB)\nInstalling collected packages: numpy-stl\nSuccessfully installed numpy-stl-3.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torch","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:14:28.190993Z","iopub.execute_input":"2024-06-24T16:14:28.191810Z","iopub.status.idle":"2024-06-24T16:14:40.254364Z","shell.execute_reply.started":"2024-06-24T16:14:28.191771Z","shell.execute_reply":"2024-06-24T16:14:40.253449Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install numpy trimesh","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:14:40.255712Z","iopub.execute_input":"2024-06-24T16:14:40.255988Z","iopub.status.idle":"2024-06-24T16:14:53.105387Z","shell.execute_reply.started":"2024-06-24T16:14:40.255961Z","shell.execute_reply":"2024-06-24T16:14:53.104308Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nCollecting trimesh\n  Downloading trimesh-4.4.1-py3-none-any.whl.metadata (18 kB)\nDownloading trimesh-4.4.1-py3-none-any.whl (694 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m694.7/694.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trimesh\nSuccessfully installed trimesh-4.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom stl import mesh\n\n# Define the 8 vertices of the cube\nvertices = np.array([\\\n    [-1, -1, -1],\n    [+1, -1, -1],\n    [+1, +1, -1],\n    [-1, +1, -1],\n    [-1, -1, +1],\n    [+1, -1, +1],\n    [+1, +1, +1],\n    [-1, +1, +1]])\n# Define the 12 triangles composing the cube\nfaces = np.array([\\\n    [0,3,1],\n    [1,3,2],\n    [0,4,7],\n    [0,7,3],\n    [4,5,6],\n    [4,6,7],\n    [5,1,2],\n    [5,2,6],\n    [2,3,6],\n    [3,7,6],\n    [0,1,5],\n    [0,5,4]])\n\n# Create the mesh\ncube = mesh.Mesh(np.zeros(faces.shape[0], dtype=mesh.Mesh.dtype))\nfor i, f in enumerate(faces):\n    for j in range(3):\n        cube.vectors[i][j] = vertices[f[j],:]\n\n# Write the mesh to file \"cube.stl\"\ncube.save('cube.stl')\nsave_path = '/path/to/your/directory/cube.stl'\n\n# Write the mesh to the specified file path\ncube.save(save_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:14:53.108346Z","iopub.execute_input":"2024-06-24T16:14:53.108749Z","iopub.status.idle":"2024-06-24T16:14:53.144140Z","shell.execute_reply.started":"2024-06-24T16:14:53.108711Z","shell.execute_reply":"2024-06-24T16:14:53.143311Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Define the path of the directory you want to create\ndirectory_path = '/kaggle/working/cubedataset'\n\n# Create the directory\nos.makedirs(directory_path, exist_ok=True)\n\nprint(f'Directory \"{directory_path}\" created successfully.')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:14:53.145273Z","iopub.execute_input":"2024-06-24T16:14:53.145832Z","iopub.status.idle":"2024-06-24T16:14:53.151419Z","shell.execute_reply.started":"2024-06-24T16:14:53.145800Z","shell.execute_reply":"2024-06-24T16:14:53.150429Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Directory \"/kaggle/working/cubedataset\" created successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom stl import mesh\nimport os\n\n# Function to create a cube mesh of given size\ndef create_cube_mesh(size):\n    # Define the vertices of the cube\n    vertices = np.array([\n        [-size, -size, -size],\n        [+size, -size, -size],\n        [+size, +size, -size],\n        [-size, +size, -size],\n        [-size, -size, +size],\n        [+size, -size, +size],\n        [+size, +size, +size],\n        [-size, +size, +size]\n    ])\n\n    # Define the faces of the cube\n    faces = np.array([\n        [0, 3, 1],\n        [1, 3, 2],\n        [0, 4, 7],\n        [0, 7, 3],\n        [4, 5, 6],\n        [4, 6, 7],\n        [5, 1, 2],\n        [5, 2, 6],\n        [2, 3, 6],\n        [3, 7, 6],\n        [0, 1, 5],\n        [0, 5, 4]\n    ])\n\n    # Create the mesh\n    cube = mesh.Mesh(np.zeros(faces.shape[0], dtype=mesh.Mesh.dtype))\n    for i, f in enumerate(faces):\n        for j in range(3):\n            cube.vectors[i][j] = vertices[f[j], :]\n\n    return cube\n\n# Directory where STL files will be saved\nsave_directory = '/kaggle/working/cubedataset'\n\n# Create cubes of sizes from 1 to 50 units and save them as STL files\nfor size in range(1, 51):\n    cube_mesh = create_cube_mesh(size)\n    filename = f'cube_size_{size}.stl'\n    file_path = os.path.join(save_directory, filename)\n    cube_mesh.save(file_path)\n    print(f'Saved {filename} at {file_path}')\n\nprint('All cubes saved successfully.')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:17:00.313519Z","iopub.execute_input":"2024-06-24T16:17:00.313865Z","iopub.status.idle":"2024-06-24T16:17:00.355330Z","shell.execute_reply.started":"2024-06-24T16:17:00.313835Z","shell.execute_reply":"2024-06-24T16:17:00.354469Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Saved cube_size_1.stl at /kaggle/working/cubedataset/cube_size_1.stl\nSaved cube_size_2.stl at /kaggle/working/cubedataset/cube_size_2.stl\nSaved cube_size_3.stl at /kaggle/working/cubedataset/cube_size_3.stl\nSaved cube_size_4.stl at /kaggle/working/cubedataset/cube_size_4.stl\nSaved cube_size_5.stl at /kaggle/working/cubedataset/cube_size_5.stl\nSaved cube_size_6.stl at /kaggle/working/cubedataset/cube_size_6.stl\nSaved cube_size_7.stl at /kaggle/working/cubedataset/cube_size_7.stl\nSaved cube_size_8.stl at /kaggle/working/cubedataset/cube_size_8.stl\nSaved cube_size_9.stl at /kaggle/working/cubedataset/cube_size_9.stl\nSaved cube_size_10.stl at /kaggle/working/cubedataset/cube_size_10.stl\nSaved cube_size_11.stl at /kaggle/working/cubedataset/cube_size_11.stl\nSaved cube_size_12.stl at /kaggle/working/cubedataset/cube_size_12.stl\nSaved cube_size_13.stl at /kaggle/working/cubedataset/cube_size_13.stl\nSaved cube_size_14.stl at /kaggle/working/cubedataset/cube_size_14.stl\nSaved cube_size_15.stl at /kaggle/working/cubedataset/cube_size_15.stl\nSaved cube_size_16.stl at /kaggle/working/cubedataset/cube_size_16.stl\nSaved cube_size_17.stl at /kaggle/working/cubedataset/cube_size_17.stl\nSaved cube_size_18.stl at /kaggle/working/cubedataset/cube_size_18.stl\nSaved cube_size_19.stl at /kaggle/working/cubedataset/cube_size_19.stl\nSaved cube_size_20.stl at /kaggle/working/cubedataset/cube_size_20.stl\nSaved cube_size_21.stl at /kaggle/working/cubedataset/cube_size_21.stl\nSaved cube_size_22.stl at /kaggle/working/cubedataset/cube_size_22.stl\nSaved cube_size_23.stl at /kaggle/working/cubedataset/cube_size_23.stl\nSaved cube_size_24.stl at /kaggle/working/cubedataset/cube_size_24.stl\nSaved cube_size_25.stl at /kaggle/working/cubedataset/cube_size_25.stl\nSaved cube_size_26.stl at /kaggle/working/cubedataset/cube_size_26.stl\nSaved cube_size_27.stl at /kaggle/working/cubedataset/cube_size_27.stl\nSaved cube_size_28.stl at /kaggle/working/cubedataset/cube_size_28.stl\nSaved cube_size_29.stl at /kaggle/working/cubedataset/cube_size_29.stl\nSaved cube_size_30.stl at /kaggle/working/cubedataset/cube_size_30.stl\nSaved cube_size_31.stl at /kaggle/working/cubedataset/cube_size_31.stl\nSaved cube_size_32.stl at /kaggle/working/cubedataset/cube_size_32.stl\nSaved cube_size_33.stl at /kaggle/working/cubedataset/cube_size_33.stl\nSaved cube_size_34.stl at /kaggle/working/cubedataset/cube_size_34.stl\nSaved cube_size_35.stl at /kaggle/working/cubedataset/cube_size_35.stl\nSaved cube_size_36.stl at /kaggle/working/cubedataset/cube_size_36.stl\nSaved cube_size_37.stl at /kaggle/working/cubedataset/cube_size_37.stl\nSaved cube_size_38.stl at /kaggle/working/cubedataset/cube_size_38.stl\nSaved cube_size_39.stl at /kaggle/working/cubedataset/cube_size_39.stl\nSaved cube_size_40.stl at /kaggle/working/cubedataset/cube_size_40.stl\nSaved cube_size_41.stl at /kaggle/working/cubedataset/cube_size_41.stl\nSaved cube_size_42.stl at /kaggle/working/cubedataset/cube_size_42.stl\nSaved cube_size_43.stl at /kaggle/working/cubedataset/cube_size_43.stl\nSaved cube_size_44.stl at /kaggle/working/cubedataset/cube_size_44.stl\nSaved cube_size_45.stl at /kaggle/working/cubedataset/cube_size_45.stl\nSaved cube_size_46.stl at /kaggle/working/cubedataset/cube_size_46.stl\nSaved cube_size_47.stl at /kaggle/working/cubedataset/cube_size_47.stl\nSaved cube_size_48.stl at /kaggle/working/cubedataset/cube_size_48.stl\nSaved cube_size_49.stl at /kaggle/working/cubedataset/cube_size_49.stl\nSaved cube_size_50.stl at /kaggle/working/cubedataset/cube_size_50.stl\nAll cubes saved successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom stl import mesh\n\nexpected_vertex_count = 108  # Expected count of vertices\n\ndef preprocess_stl(file_path, expected_count):\n    # Load the STL file\n    your_mesh = mesh.Mesh.from_file(file_path)\n    vertices = your_mesh.vectors.reshape(-1, 3)\n    \n    # Check if resizing or padding is needed\n    current_count = vertices.shape[0]\n    if current_count != expected_count:\n        print(f\"Warning: Vertices shape mismatch for file {file_path}. Expected {expected_count}, got {current_count}. Resizing vertices.\")\n        if current_count < expected_count:\n            # Pad with zeros if fewer vertices than expected\n            pad_size = expected_count - current_count\n            padding = np.zeros((pad_size, 3))\n            vertices = np.vstack((vertices, padding))\n        else:\n            # Truncate if more vertices than expected\n            vertices = vertices[:expected_count]\n    \n    return vertices\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:21:18.240685Z","iopub.execute_input":"2024-06-24T16:21:18.241521Z","iopub.status.idle":"2024-06-24T16:21:18.249048Z","shell.execute_reply.started":"2024-06-24T16:21:18.241486Z","shell.execute_reply":"2024-06-24T16:21:18.248013Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport trimesh\nimport numpy as np\n\nclass STL3DDataset(Dataset):\n    def __init__(self, file_list, expected_count=13824):\n        self.file_list = file_list\n        self.expected_count = expected_count\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        file_path = self.file_list[idx]\n        mesh = trimesh.load(file_path)\n\n        vertices = mesh.vertices\n        current_count = vertices.shape[0]\n        if current_count != self.expected_count:\n            print(f\"Warning: Vertices shape mismatch for file {file_path}. Expected {self.expected_count}, got {current_count}. Resizing vertices.\")\n            if current_count < self.expected_count:\n                pad_size = self.expected_count - current_count\n                padding = np.zeros((pad_size, 3))\n                vertices = np.vstack((vertices, padding))\n            else:\n                vertices = vertices[:self.expected_count]\n\n        vertices = torch.tensor(vertices, dtype=torch.float32)\n        vertices = vertices.flatten()\n\n        return vertices\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:21:50.798693Z","iopub.execute_input":"2024-06-24T16:21:50.799555Z","iopub.status.idle":"2024-06-24T16:21:50.808458Z","shell.execute_reply.started":"2024-06-24T16:21:50.799522Z","shell.execute_reply":"2024-06-24T16:21:50.807515Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom torchsummary import summary\nfrom tqdm import tqdm\nimport trimesh\n\n\n# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:18:07.284406Z","iopub.execute_input":"2024-06-24T16:18:07.285221Z","iopub.status.idle":"2024-06-24T16:18:07.998404Z","shell.execute_reply.started":"2024-06-24T16:18:07.285183Z","shell.execute_reply":"2024-06-24T16:18:07.997453Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:17:32.809379Z","iopub.execute_input":"2024-06-24T16:17:32.810419Z","iopub.status.idle":"2024-06-24T16:17:45.054682Z","shell.execute_reply.started":"2024-06-24T16:17:32.810375Z","shell.execute_reply":"2024-06-24T16:17:45.053544Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(100, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 13824*3),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        output = self.model(z)\n        return output.view(-1, 13824, 3)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(13824*3, 1024),\n            nn.LeakyReLU(0.2),\n            nn.Linear(1024, 512),\n            nn.LeakyReLU(0.2),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, vertices):\n        vertices = vertices.view(vertices.size(0), -1)\n        output = self.model(vertices)\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:22:22.087689Z","iopub.execute_input":"2024-06-24T16:22:22.088327Z","iopub.status.idle":"2024-06-24T16:22:22.097940Z","shell.execute_reply.started":"2024-06-24T16:22:22.088294Z","shell.execute_reply":"2024-06-24T16:22:22.096897Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_file_list(directory):\n    file_list = []\n    for filename in os.listdir(directory):\n        if filename.endswith('.stl'):\n            file_list.append(os.path.join(directory, filename))\n    return file_list\n\ndata_directory = \"/kaggle/working/cubedataset/\"\nfile_list = get_file_list(data_directory)\ndataset = STL3DDataset(file_list)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(100, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 13824*3),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        output = self.model(z)\n        return output.view(-1, 13824, 3)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:23:58.345886Z","iopub.execute_input":"2024-06-24T16:23:58.346665Z","iopub.status.idle":"2024-06-24T16:23:58.725862Z","shell.execute_reply.started":"2024-06-24T16:23:58.346628Z","shell.execute_reply":"2024-06-24T16:23:58.724450Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m STL3DDataset(file_list)\n\u001b[1;32m      8\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vertices \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(vertices\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should be [batch_size, 13824*3]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36mSTL3DDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     15\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list[idx]\n\u001b[0;32m---> 16\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m \u001b[43mtrimesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     vertices \u001b[38;5;241m=\u001b[39m mesh\u001b[38;5;241m.\u001b[39mvertices\n\u001b[1;32m     19\u001b[0m     current_count \u001b[38;5;241m=\u001b[39m vertices\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trimesh/exchange/load.py:114\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file_obj, file_type, resolver, force, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file_obj\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# parse the file arguments into clean loadable form\u001b[39;00m\n\u001b[1;32m    108\u001b[0m (\n\u001b[1;32m    109\u001b[0m     file_obj,  \u001b[38;5;66;03m# file- like object\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     file_type,  \u001b[38;5;66;03m# str, what kind of file\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     metadata,  \u001b[38;5;66;03m# dict, any metadata from file name\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     opened,  \u001b[38;5;66;03m# bool, did we open the file ourselves\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     resolver,  \u001b[38;5;66;03m# object to load referenced resources\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_file_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file_obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;66;03m# if we've been passed a dict treat it as kwargs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trimesh/exchange/load.py:612\u001b[0m, in \u001b[0;36m_parse_file_args\u001b[0;34m(file_obj, file_type, resolver, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse load_remote to load URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring is not a file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m     file_type \u001b[38;5;241m=\u001b[39m file_obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n","\u001b[0;31mValueError\u001b[0m: string is not a file: path/to/stl1.stl"],"ename":"ValueError","evalue":"string is not a file: path/to/stl1.stl","output_type":"error"}]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(13824*3, 1024),\n            nn.LeakyReLU(0.2),\n            nn.Linear(1024, 512),\n            nn.LeakyReLU(0.2),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, vertices):\n        vertices = vertices.view(vertices.size(0), -1)\n        output = self.model(vertices)\n        return output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\nsummary(generator, (100,))\nsummary(discriminator, (13824*3,))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.BCELoss()\noptimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 50\nfor epoch in range(num_epochs):\n    for i, real_vertices in enumerate(dataloader):\n        real_vertices = real_vertices.to(device)\n        batch_size = real_vertices.size(0)\n\n        # Create labels for real and fake data\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        # Train Discriminator\n        optimizer_D.zero_grad()\n        outputs = discriminator(real_vertices)\n        d_loss_real = criterion(outputs, real_labels)\n        d_loss_real.backward()\n\n        z = torch.randn(batch_size, 100).to(device)\n        fake_vertices = generator(z)\n        outputs = discriminator(fake_vertices.detach())\n        d_loss_fake = criterion(outputs, fake_labels)\n        d_loss_fake.backward()\n        optimizer_D.step()\n\n        d_loss = d_loss_real + d_loss_fake\n\n        # Train Generator\n        optimizer_G.zero_grad()\n        z = torch.randn(batch_size, 100).to(device)\n        fake_vertices = generator(z)\n        outputs = discriminator(fake_vertices)\n        g_loss = criterion(outputs, real_labels)\n        g_loss.backward()\n        optimizer_G.step()\n\n        if (i + 1) % 1 == 0:\n            print(f'Epoch [{epoch}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pymeshlab import MeshSet\n\ndef remesh_stl(file_path, target_vertex_count):\n    ms = MeshSet()\n    ms.load_new_mesh(file_path)\n    ms.meshing_isotropic_explicit_remeshing(targetlen=1.0, exactnumvert=target_vertex_count)\n    ms.save_current_mesh(file_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CubeDataset(Dataset):\n    def __init__(self, directory, cube_dim=24):\n        self.file_list = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.stl')]\n        self.cube_dim = cube_dim\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        filename = self.file_list[idx]\n        cube_mesh = mesh.Mesh.from_file(filename)\n        vertices = cube_mesh.vectors.reshape(-1)\n\n        expected_size = self.cube_dim ** 3 * 3\n        if vertices.shape[0] != expected_size:\n            print(f\"Warning: Vertices shape mismatch for file {filename}. Expected {expected_size}, got {vertices.shape[0]}. Resizing vertices.\")\n            if vertices.shape[0] < expected_size:\n                vertices = np.pad(vertices, (0, expected_size - vertices.shape[0]), mode='constant', constant_values=0)\n            else:\n                vertices = vertices[:expected_size]\n\n        return vertices.astype(np.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, output_dim):\n        super(Generator, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(latent_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, output_dim),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        return self.fc(z)\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_dim):\n        super(Discriminator, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\nclass CubeGAN:\n    def __init__(self, directory, latent_dim=100, cube_dim=24, batch_size=64, lr=0.0002):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.latent_dim = latent_dim\n        self.cube_dim = cube_dim\n\n        self.generator = Generator(latent_dim, cube_dim**3 * 3).to(self.device)\n        self.discriminator = Discriminator(cube_dim**3 * 3).to(self.device)\n\n        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n        self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n\n        self.criterion = nn.BCELoss()\n\n        self.dataset = CubeDataset(directory, cube_dim=cube_dim)\n        self.dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\n    def train(self, num_epochs):\n        for epoch in range(num_epochs):\n            for i, real_cubes in enumerate(self.dataloader):\n                real_cubes = real_cubes.to(self.device)\n\n                valid = torch.ones(real_cubes.size(0), 1).to(self.device)\n                fake = torch.zeros(real_cubes.size(0), 1).to(self.device)\n\n                # Train Discriminator\n                self.optimizer_D.zero_grad()\n\n                real_loss = self.criterion(self.discriminator(real_cubes), valid)\n\n                z = torch.randn(real_cubes.size(0), self.latent_dim).to(self.device)\n                fake_cubes = self.generator(z)\n\n                fake_loss = self.criterion(self.discriminator(fake_cubes.detach()), fake)\n\n                d_loss = real_loss + fake_loss\n                d_loss.backward()\n                self.optimizer_D.step()\n\n                # Train Generator\n                self.optimizer_G.zero_grad()\n\n                g_loss = self.criterion(self.discriminator(fake_cubes), valid)\n                g_loss.backward()\n                self.optimizer_G.step()\n\n                if i % 10 == 0:\n                    print(f'Epoch [{epoch}/{num_epochs}], Batch [{i}/{len(self.dataloader)}], '\n                          f'D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}')\n\n    def generate_cube(self, cube_size):\n        with torch.no_grad():\n            self.generator.eval()\n            z = torch.randn(1, self.latent_dim).to(self.device)\n            fake_cube = self.generator(z).cpu().numpy().reshape(-1, 3) * cube_size\n            return fake_cube\n\ndef get_user_input():\n    cube_size = int(input(\"Enter the dimension of the cube you want to generate: \"))\n    return cube_size\n\nif __name__ == '__main__':\n    stl_directory = '/kaggle/working/cubedataset/'\n    cube_gan = CubeGAN(stl_directory)\n    cube_gan.train(num_epochs=50)\n\n    cube_size = get_user_input()\n    generated_cube = cube_gan.generate_cube(cube_size)\n    print(f'Generated Cube with dimension {cube_size}:')\n    print(generated_cube)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}